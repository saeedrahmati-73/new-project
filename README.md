# new-project
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import sys
import time        
import logging
import warnings
import multiprocessing as mp
from datetime import datetime, timedelta

import pandas as pd
import numpy as np
import requests
from retrying import retry
from persiantools.jdatetime import JalaliDate
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ==== Û±) Ø®Ù†Ø«ÛŒâ€ŒØ³Ø§Ø²ÛŒ Ù¾Ø±Ø§Ú©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ ====
for _v in ("HTTP_PROXY", "HTTPS_PROXY", "http_proxy", "https_proxy", "ALL_PROXY", "all_proxy"):
    os.environ.pop(_v, None)
os.environ["NO_PROXY"] = "*"

# ==== Û²) ØªÙ†Ø¸ÛŒÙ… Ù„Ø§Ú¯ÛŒÙ†Ú¯ ====
logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)
warnings.filterwarnings("ignore", category=UserWarning)


def fetch_stock_raw(symbol: str) -> str:
    """
    Ø¯Ø±Ø®ÙˆØ§Ø³Øª HTTP Ø¨Ù‡ TSETMC Ù…ÛŒâ€ŒØ²Ù†Ø¯ Ùˆ Ù…ØªÙ† Ø®Ø§Ù… Ù¾Ø§Ø³Ø® Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
    """
    url = f"http://www.tsetmc.com/Loader.aspx?ParTree=151311&i={symbol}"
    headers = {
        'User-Agent': (
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
            'AppleWebKit/537.36 (KHTML, like Gecko) '
            'Chrome/115.0.0.0 Safari/537.36'
        ),
        'Referer': 'http://www.tsetmc.com/Loader.aspx'
    }

    logger.debug("Sending HTTP GET â†’ %s", url)
    resp = requests.get(url, headers=headers, timeout=10)
    logger.debug(
        "HTTP %s %s â†’ status=%s",
        resp.request.method,
        resp.url,
        resp.status_code
    )
    # Ù„Ø§Ú¯ ÛµÛ°Û° Ú©Ø§Ø±Ø§Ú©ØªØ± Ø§ÙˆÙ„ Ù…ØªÙ† Ù¾Ø§Ø³Ø® Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØ¨Ø§Ú¯
    logger.debug("RAW RESPONSE TEXT (first 500 chars):%s", resp.text[:500])

    resp.raise_for_status()
    return resp.text


class TehranStockAPI:
    """
    Ú©Ù„Ø§Ø³ API Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡ Ø®Ø§Ù… Ø§Ø² TSETMC Ùˆ Ø¬Ø³ØªØ¬ÙˆÛŒ Ù†Ù…Ø§Ø¯Ù‡Ø§
    Ø¨Ø§ ØºÛŒØ±ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù† Ù¾Ø±Ø§Ú©Ø³ÛŒ Ùˆ Ù…Ú©Ø§Ù†ÛŒØ²Ù… Retry
    """
    def __init__(self):
        self.base_url = "http://www.tsetmc.com/tsev2/data"
        # Ø³Ø§Ø®Øª Session Ø¨Ø¯ÙˆÙ† Ø§ØªÚ©Ø§ Ø¨Ù‡ Ù¾Ø±Ø§Ú©Ø³ÛŒ Ù…Ø­ÛŒØ·ÛŒ
        self.session = requests.Session()
        self.session.trust_env = False
        self.session.proxies.clear()

        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Retry Ø¨Ù‡ Session
        retry_strategy = Retry(
            total=3,
            backoff_factor=0.5,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET"]
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)

        # Ù‡Ø¯Ø±Ù‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡
        self.headers_list = [
            {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                              'AppleWebKit/537.36 (KHTML, like Gecko) '
                              'Chrome/121.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'fa-IR,fa;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Cache-Control': 'no-cache',
                'Pragma': 'no-cache'
            }
        ]

        # Ú©Ø´ Ø§ÙˆÙ„ÛŒÙ‡â€ŒÛŒ Ù†Ù…Ø§Ø¯Ù‡Ø§
        self.known_symbols = {
            'ÙÙˆÙ„Ø§Ø¯': '46348559193224090',
            'Ø®ÙˆØ¯Ø±Ùˆ': '65883838195688438',
            'Ø´Ù¾Ù†Ø§': '778253364357513',
            'Ù¾Ø§Ø±Ø³': '35700344742885862',
            'Ú©Ú¯Ù„': '17635693434652166',
            'ÙˆØ¨Ù…Ù„Øª': '35425587644337450',
            'ÙÙ…Ù„ÛŒ': '43685683301925375'
        }

    def search_symbol(self, symbol_name: str) -> str | None:
        """Ø¬Ø³ØªØ¬ÙˆÛŒ Ú©Ø¯ Ù†Ù…Ø§Ø¯ Ø¨Ø§ fallback Ø¨Ù‡ Ú©Ø´ Ù…Ø­Ù„ÛŒ"""
        logger.info("ğŸ” Ø¬Ø³ØªØ¬ÙˆÛŒ Ù†Ù…Ø§Ø¯: %s", symbol_name)
        if symbol_name in self.known_symbols:
            code = self.known_symbols[symbol_name]
            logger.info("âœ… Ú©Ø¯ Ø§Ø² Ú©Ø´: %s", code)
            return code

        try:
            headers = np.random.choice(self.headers_list)
            url = f"{self.base_url}/search.aspx"
            params = {'skey': symbol_name}
            resp = self.session.get(
                url, params=params, headers=headers,
                timeout=10, allow_redirects=True
            )
            if resp.status_code == 200 and resp.text.strip():
                part = resp.text.split(';')[0]
                if ',' in part:
                    code = part.split(',')[0]
                    self.known_symbols[symbol_name] = code
                    logger.info("âœ… Ú©Ø¯ Ø¢Ù†Ù„Ø§ÛŒÙ† ÛŒØ§ÙØª Ø´Ø¯: %s", code)
                    return code
        except Exception as e:
            logger.error("âŒ Ø®Ø·Ø§ Ø¯Ø± search_symbol: %s", e)

        logger.error("âŒ Ù†Ù…Ø§Ø¯ ÛŒØ§ÙØª Ù†Ø´Ø¯")
        return None

    def get_stock_data(self, symbol_code: str) -> pd.DataFrame | None:
        """Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡ Ø®Ø§Ù… Ø³Ù‡Ø§Ù… Ø§Ø² API TSETMC"""
        if not symbol_code:
            logger.error("âŒ Ù†Ù…Ø§Ø¯ Ù…Ø¹ØªØ¨Ø± Ù†ÛŒØ³Øª")
            return None

        logger.info("ğŸ“Š Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ú©Ø¯: %s", symbol_code)
        try:
            headers = np.random.choice(self.headers_list)
            url = f"{self.base_url}/Export-txt.aspx"
            params = {'i': symbol_code, 't': 'i', 'a': '0'}
            # ØªØ§Ø®ÛŒØ± ØªØµØ§Ø¯ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø¨Ù„Ø§Ú© Ø´Ø¯Ù†
            time.sleep(1 + np.random.random())
            resp = self.session.get(
                url, params=params, headers=headers,
                timeout=15, allow_redirects=True
            )

            logger.debug(
                "HTTP %s %s â†’ status=%s",
                resp.request.method,
                resp.url,
                resp.status_code
            )
            if resp.status_code != 200:
                logger.error("âŒ Ú©Ø¯ ÙˆØ¶Ø¹ÛŒØª HTTP: %s", resp.status_code)
                return None

            text = resp.text
            # Ø§Ú¯Ø± HTML Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù‡ Ø´Ø¯ ÛŒØ¹Ù†ÛŒ Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ Ø¨Ù„Ø§Ú© Ø´Ø¯Ù‡â€ŒØ§ÛŒÙ…
            if '<html>' in text.lower():
                logger.error("âŒ Ø¯Ø³ØªØ±Ø³ÛŒ Ù…Ø³Ø¯ÙˆØ¯ Ø´Ø¯ ÛŒØ§ ØµÙØ­Ù‡ HTML Ø¨Ø§Ø²Ú¯Ø´Øª Ø¯Ø§Ø¯Ù‡ Ø´Ø¯")
                return None

            return self._parse_stock_data(text)

        except Exception as e:
            logger.error("âŒ Ø®Ø·Ø§ Ø¯Ø± get_stock_data: %s", e)
            return None

    def _parse_stock_data(self, raw: str) -> pd.DataFrame | None:
        """ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø®Ø§Ù… API Ø¨Ù‡ DataFrame"""
        try:
            if not raw.strip():
                logger.error("âŒ Ø¯Ø§Ø¯Ù‡ Ø®Ø§Ù… Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø±Ø³ Ú©Ø±Ø¯Ù† Ø®Ø§Ù„ÛŒ Ø§Ø³Øª")
                return None

            logger.debug("Parsing raw data (first 200 chars):%s", raw[:200])
            lines = [l.strip() for l in raw.splitlines() if l.strip()]
            data_list = []

            for line in lines:
                parts = line.split(',')
                if len(parts) < 6:
                    continue

                date_str = parts[0]
                # Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø³Ø§Ø¯Ù‡ ØªØ§Ø±ÛŒØ®
                if len(date_str) != 8 or not date_str.isdigit():
                    continue
                y, m, d = map(int, (date_str[:4], date_str[4:6], date_str[6:8]))
                if not (1300 <= y <= 1500 and 1 <= m <= 12 and 1 <= d <= 31):
                    continue

                try:
                    dt = datetime(y, m, d)
                    open_, high, low, close = map(float, parts[1:5])
                    volume = int(float(parts[5])) if parts[5].strip() else 0
                except ValueError:
                    continue

                data_list.append({
                    'Date': dt,
                    'Open': open_,
                    'High': high,
                    'Low': low,
                    'Close': close,
                    'Volume': volume
                })

            if not data_list:
                logger.error("âŒ Ù‡ÛŒÚ† Ø±Ú©ÙˆØ±Ø¯ Ù…Ø¹ØªØ¨Ø±ÛŒ Ù¾Ø³ Ø§Ø² Ù¾Ø§Ø±Ø³ Ù†Ø´Ø¯")
                return None

            df = pd.DataFrame(data_list)
            df = df.drop_duplicates('Date').sort_values('Date').reset_index(drop=True)
            logger.info("âœ… %d Ø±Ú©ÙˆØ±Ø¯ Ù¾Ø§Ø±Ø³ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯", len(df))
            return df

        except Exception as e:
            logger.error("âŒ Ø®Ø·Ø§ Ø¯Ø± _parse_stock_data: %s", e)
            logger.debug("RAW CONTENT (200 chars): %r", raw[:200])
            return None


class TehranStockDataManager:
    """
    Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ø´ Ùˆ ØªØ±Ú©ÛŒØ¨ Ø¯Ø§Ø¯Ù‡ Ø®Ø§Ù… Ø¨Ø§ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒØŒ
    Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø¢ÙÙ„Ø§ÛŒÙ†/Ø¢Ù†Ù„Ø§ÛŒÙ† Ùˆ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡
    """
    def __init__(self, cache_dir="cache", cache_expiry_days=7):
        self.cache_dir = cache_dir
        self.cache_expiry_days = cache_expiry_days
        self.api = TehranStockAPI()
        self._ensure_cache_dir()
        logger.info("âœ… TehranStockDataManager Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯")

    def _ensure_cache_dir(self):
        if not os.path.isdir(self.cache_dir):
            os.makedirs(self.cache_dir, exist_ok=True)
            logger.info("ğŸ“ Ù¾ÙˆØ´Ù‡ Ú©Ø´ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯: %s", self.cache_dir)

    def _cache_path(self, symbol: str, days: int) -> str:
        return os.path.join(self.cache_dir, f"{symbol}_{days}.parquet")

    def _is_cache_valid(self, path: str) -> bool:
        if not os.path.exists(path):
            return False
        age = (datetime.now() - datetime.fromtimestamp(os.path.getmtime(path))).days
        return age <= self.cache_expiry_days

    def _load_from_cache(self, symbol: str, days: int) -> pd.DataFrame | None:
        path = self._cache_path(symbol, days)
        if not os.path.exists(path):
            return None
        try:
            if self._is_cache_valid(path):
                df = pd.read_parquet(path, engine='fastparquet')
                logger.info("âœ… Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø§Ø² Ú©Ø´: %s (%d Ø±Ú©ÙˆØ±Ø¯)", path, len(df))
                return df
        except Exception as e:
            logger.error("âŒ Ø®Ø·Ø§ Ø¯Ø± load_from_cache: %s", e)
        return None

    def _save_to_cache(self, symbol: str, days: int, df: pd.DataFrame):
        path = self._cache_path(symbol, days)
        try:
            # ØªÙ„Ø§Ø´ Ø°Ø®ÛŒØ±Ù‡ Ø¨Ø§ fastparquet
            df.to_parquet(path, engine='fastparquet', compression='snappy')
            logger.info("ğŸ’¾ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ú©Ø´ (fastparquet): %s", path)
        except Exception as e_fast:
            logger.warning("âš ï¸ Ø°Ø®ÛŒØ±Ù‡ Ø¨Ø§ fastparquet Ù…ÙˆÙÙ‚ Ù†Ø¨ÙˆØ¯: %s", e_fast)
            try:
                df.to_parquet(path, engine='pyarrow', compression='snappy')
                logger.info("ğŸ’¾ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ú©Ø´ (pyarrow): %s", path)
            except Exception as e_arrow:
                logger.error("âŒ Ø°Ø®ÛŒØ±Ù‡ Ú©Ø´ Ø¨Ø§ pyarrow Ù‡Ù… Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯: %s", e_arrow)
                try:
                    csv_path = path.replace('.parquet', '.csv')
                    df.to_csv(csv_path, index=False)
                    logger.info("ğŸ’¾ Ø°Ø®ÛŒØ±Ù‡ Ø§Ø¶Ø·Ø±Ø§Ø±ÛŒ Ø¯Ø± CSV: %s", csv_path)
                except Exception as e_csv:
                    logger.error("âŒ Ø°Ø®ÛŒØ±Ù‡ Ø§Ø¶Ø·Ø±Ø§Ø±ÛŒ CSV Ù‡Ù… Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯: %s", e_csv)

    def clear_cache(self, symbol: str | None = None):
        try:
            files = os.listdir(self.cache_dir)
            for f in files:
                if symbol is None or f.startswith(f"{symbol}_"):
                    os.remove(os.path.join(self.cache_dir, f))
            logger.info("ğŸ—‘ï¸ Ú©Ø´ %s Ù¾Ø§Ú© Ø´Ø¯", symbol or "Ù‡Ù…Ù‡")
        except Exception as e:
            logger.error("âŒ Ø®Ø·Ø§ Ø¯Ø± clear_cache: %s", e)

    def _load_from_api(self, symbol: str, days: int) -> pd.DataFrame | None:
        code = self.api.search_symbol(symbol)
        if not code:
            return None
        df = self.api.get_stock_data(code)
        if df is None or df.empty:
            return None
        return df.tail(days)

    @retry(stop_max_attempt_number=2,
           wait_exponential_multiplier=300,
           wait_exponential_max=3000)
    def get_stock_data(self, symbol: str, days: int = 300, online: bool = True) -> pd.DataFrame | None:
        logger.info(
            "ğŸ“Š Ø¯Ø±Ø®ÙˆØ§Ø³Øª %d Ø±ÙˆØ² Ø¨Ø±Ø§ÛŒ %s - Ø­Ø§Ù„Øª: %s",
            days, symbol, "Ø¢Ù†Ù„Ø§ÛŒÙ†" if online else "Ø¢ÙÙ„Ø§ÛŒÙ†"
        )

        # Û±) ØªÙ„Ø§Ø´ Ø§Ø² Ú©Ø´
        cached = self._load_from_cache(symbol, days)
        if cached is not None and len(cached) >= 10:
            return cached

        # Û²) Ø±ÙˆØ´ Ø¢Ù†Ù„Ø§ÛŒÙ†
        if online:
            df_api = self._load_from_api(symbol, days)
            if df_api is not None and len(df_api) >= 10:
                df_prep = self._prepare_data(df_api)
                self._save_to_cache(symbol, days, df_prep)
                return df_prep
            logger.warning("âš ï¸ Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù†Ù„Ø§ÛŒÙ† Ù†Ø§Ù…ÙˆÙÙ‚ØŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡")

        else:
            logger.info("âš ï¸ Ø­Ø§Ù„Øª Ø¢ÙÙ„Ø§ÛŒÙ†: Ø§Ø² Ú©Ø´ ÛŒØ§ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯")

        # Û³) Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡
        sample = self._get_sample_data(symbol, days)
        if sample is not None:
            df_prep = self._prepare_data(sample)
            self._save_to_cache(symbol, days, df_prep)
            return df_prep

        logger.error("âŒ Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯")
        return None

    def _get_sample_data(self, symbol: str, days: int) -> pd.DataFrame | None:
        """ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡ ØªØµØ§Ø¯ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø´Ø±Ø§ÛŒØ· fallback"""
        try:
            end = datetime.now()
            start = end - timedelta(days=int(days * 1.5))
            all_dates = pd.date_range(start, end, freq='D')
            biz = all_dates[all_dates.weekday < 5]
            dates = biz[-days:] if len(biz) >= days else biz

            base = 1000 + (hash(symbol) % 10000)
            n = len(dates)
            np.random.seed(hash(symbol) % 2**32)
            returns = np.random.normal(0.001, 0.015, n)
            prices = np.cumprod(1 + returns) * base
            prices = np.clip(prices, 100, None)
            vol = np.random.randint(100_000, 600_000, n)

            return pd.DataFrame({
                'Date': dates,
                'Open': prices * np.random.uniform(0.98, 1.02, n),
                'High': prices * np.random.uniform(1.00, 1.05, n),
                'Low':  prices * np.random.uniform(0.95, 1.00, n),
                'Close': prices,
                'Volume': vol
            })
        except Exception as e:
            logger.error("âŒ Ø®Ø·Ø§ Ø¯Ø± sample_data: %s", e)
            return None

    def _prepare_data(self, df: pd.DataFrame) -> pd.DataFrame | None:
        """Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒØŒ ÛŒÚ©Ø³Ø§Ù†â€ŒØ³Ø§Ø²ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ Ùˆ Ø§ÙØ²ÙˆØ¯Ù‡ Ú©Ø±Ø¯Ù† ØªØ§Ø±ÛŒØ® Ø´Ù…Ø³ÛŒ"""
        try:
            if df is None or df.empty:
                return None

            df2 = df.copy()
            # Ø³Øª Ú©Ø±Ø¯Ù† Ø§ÛŒÙ†Ø¯Ú©Ø³ ØªØ§Ø±ÛŒØ®
            if 'Date' in df2.columns:
                df2['Date'] = pd.to_datetime(df2['Date'], errors='coerce')
                df2 = df2.dropna(subset=['Date']).set_index('Date')
            else:
                df2.index = pd.to_datetime(df2.index, errors='coerce')
                df2 = df2.dropna()

            # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒ
            df2 = df2[~df2.index.duplicated(keep='first')]

            # Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
            mapping = {}
            for col in df2.columns:
                lc = col.lower()
                if lc in ['open', 'o', 'first', 'yesterday']:
                    mapping[col] = 'Open'
                elif lc in ['high', 'h', 'max']:
                    mapping[col] = 'High'
                elif lc in ['low', 'l', 'min']:
                    mapping[col] = 'Low'
                elif lc in ['close', 'c', 'last', 'final']:
                    mapping[col] = 'Close'
                elif lc in ['volume', 'vol', 'count', 'value', 'v']:
                    mapping[col] = 'Volume'

            df2 = df2.rename(columns=mapping)

            # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ Ù‡Ù…Ù‡ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²
            for req in ['Open', 'High', 'Low', 'Close', 'Volume']:
                if req not in df2.columns:
                    df2[req] = 0

            # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø¹Ø¯Ø¯ÛŒ
            for numcol in ['Open', 'High', 'Low', 'Close', 'Volume']:
                df2[numcol] = pd.to_numeric(df2[numcol], errors='coerce').fillna(0)

            # Ø§ÙØ²ÙˆØ¯Ù† ØªØ§Ø±ÛŒØ® Ø´Ù…Ø³ÛŒ
            df2['JalaliDate'] = [
                JalaliDate.to_jalali(d).strftime('%Y/%m/%d') for d in df2.index
            ]

            df2 = df2.sort_index()
            logger.info("âœ… Ø¯Ø§Ø¯Ù‡ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯: %d Ø±Ú©ÙˆØ±Ø¯", len(df2))
            return df2

        except Exception as e:
            logger.error("âŒ Ø®Ø·Ø§ Ø¯Ø± prepare_data: %s", e)
            return None

    def get_data_summary(self, df: pd.DataFrame | None) -> dict:
        if df is None or df.empty:
            return {}
        return {
            "shape": df.shape,
            "columns": list(df.columns),
            "describe": df.describe().to_dict()
        }

    def test_data_sources(self, symbols=None, days_list=None) -> dict:
        if symbols is None:
            symbols = ['ÙÙˆÙ„Ø§Ø¯', 'Ø®ÙˆØ¯Ø±Ùˆ']
        if days_list is None:
            days_list = [50, 100]
        results = {}
        for sym in symbols:
            for d in days_list:
                df = self.get_stock_data(sym, d, online=True)
                results[f"{sym}_{d}"] = df
                time.sleep(1)
        return results


if __name__ == "__main__":
    mgr = TehranStockDataManager()
    df_online = mgr.get_stock_data("ÙÙˆÙ„Ø§Ø¯", days=50, online=True)
    logger.info("Ø®Ù„Ø§ØµÙ‡ Ø¢Ù†Ù„Ø§ÛŒÙ†: %s", mgr.get_data_summary(df_online))
    df_offline = mgr.get_stock_data("ÙÙˆÙ„Ø§Ø¯", days=50, online=False)
    logger.info("Ø®Ù„Ø§ØµÙ‡ Ø¢ÙÙ„Ø§ÛŒÙ†: %s", mgr.get_data_summary(df_offline))
